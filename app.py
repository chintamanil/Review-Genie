# -*- coding: utf-8 -*-
"""Review_Geniepynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Tj7eKeKy0jh-a5DdvTdDBi75fticVaP

## Evaluating RAG pipelines

### **Why Evaluation for a RAG-Based Application is Necessary**

When building an **AI-powered product discovery chatbot** using **RAG**, evaluating its performance is critical to ensure **accuracy, relevance, and user experience**. Here’s why:

#### **1. Ensuring Relevant & Context-Aware Recommendations**
- Users expect **precise** and **personalized** suggestions.
- Evaluation ensures retrieved product data is **accurate, diverse, and aligned** with user intent.

#### **2. Measuring Retrieval Quality**
- The chatbot relies on fetching relevant product descriptions.
- We must assess **precision, recall, and ranking** to optimize retrieval strategies.

#### **3. Evaluating Generation for Coherence & Factuality**
- LLMs might **hallucinate** or provide incorrect product details.
- Metrics like **faithfulness to retrieved data and factual accuracy** ensure reliable responses.

#### **4. Handling Multi-Turn Conversations**
- Conversations involve **follow-ups, clarifications, and evolving preferences**.
- We need to test **context retention, coherence, and relevance** across turns.

#### **5. User-Centric Evaluation**
- Does the chatbot **reduce effort** and **improve product discovery**?
- Collecting **human ratings, engagement metrics, and qualitative feedback** ensures real-world effectiveness.

<center><img src="https://miro.medium.com/v2/resize:fit:1400/1*k3qP8mLd2NBB5Z9S9k13-A.png" width=500/></center>

By continuously evaluating retrieval, generation, and user satisfaction, we can refine the chatbot to **deliver fast, relevant, and trustworthy product recommendations**—enhancing the shopping experience.

Using the above frameworks, we observe that there are quite a few parameters which we can use to evaluate the LLM's responses. Based on well-known practices we shall focus on the following parameters for now.

(Source: LangSmith)


#### **Evaluation Criteria for RAG-Based Applications**

 **1. Correctness: Response vs Reference Answer**
- **Goal:** Measure *how similar/correct is the RAG chain answer, relative to a ground-truth answer.*
- **Mode:** Requires a ground truth (reference) answer supplied through a dataset.
- **Evaluator:** Use **LLM-as-judge** to assess answer correctness.

**2. Relevance: Response vs Input**
- **Goal:** Measure *how well does the generated response address the initial user input.*
- **Mode:** Does **not** require a reference answer, as it compares the response to the input question.
- **Evaluator:** Use **LLM-as-judge** to assess answer relevance, helpfulness, etc.

**3. Groundedness: Response vs Retrieved Docs**
- **Goal:** Measure *to what extent does the generated response agree with the retrieved context.*
- **Mode:** Does **not** require a reference answer, as it compares the response to the retrieved context.
- **Evaluator:** Use **LLM-as-judge** to assess faithfulness, hallucinations, etc.

**4. Retrieval Relevance: Retrieved Docs vs Input**
- **Goal:** Measure *how relevant are my retrieved results for this query.*
- **Mode:** Does **not** require a reference answer, as it compares the query to the retrieved context.
- **Evaluator:** Use **LLM-as-judge** to assess relevance.


Let's now go ahead and implement the above. Our code would contain the following parts:

- Building a simple RAG application for Review Genie (we can reuse the code from earlier)
- Create a evaluation set of questions and ideal answers
- Run our application on the evaluation set
- Evaluate the application's responses using multiple metrics.

Now to perform the above seamlessly, we shall use **LangSmith**

### Introduction to LangSmith

LangSmith is a debugging, monitoring, and evaluation platform for LLM-based applications, developed by the creators of LangChain. It helps developers track, test, and optimize their AI workflows by providing insights into prompt behavior, latency, token usage, and model performance. LangSmith allows for seamless integration with LangChain applications, making it easier to debug complex chains, compare model outputs, and improve reliability in production deployments.

<center><img src="https://miro.medium.com/v2/resize:fit:1400/1*fOoaEJnm2WC5S9zCFNqMJA.png" width = 500/></center>

Before proceeding further, you should create a LangSmith API key. Here are the steps to create one

- Go to https://www.langchain.com/langsmith
- You can create a new account or sign up using your Google/Github account.
- While onboarding, you will be given an API key. Make sure you copy it for further usage in this code.
- This is how your personal dashboard would look like:

### How LangSmith helps in Evaluation?

LangSmith uses **Tracing**, a powerful tool for understanding the behavior of your LLM application.LangSmith's tracing concept is designed to help developers debug and optimize LLM-based applications by capturing and visualizing the execution flow of a LangChain pipeline. Tracing records detailed information about each step in a chain, including:

1. **Input & Output Tracking** – Captures inputs, outputs, and intermediate steps in a structured format.
2. **Execution Flow Visualization** – Provides a detailed view of how prompts, functions, and models interact within a chain.
3. **Latency & Cost Analysis** – Measures execution time and token usage at each step to optimize efficiency.
4. **Error Debugging** – Identifies failures, unexpected outputs, and performance bottlenecks.
5. **Comparison & Experimentation** – Enables versioning and side-by-side comparisons of different prompts, models, or chain configurations.

Tracing in LangSmith allows developers to gain deeper insights into their LLM pipelines, making it easier to debug, refine, and scale applications.

<center><img src="https://docs.smith.langchain.com/assets/images/primitives-708e671bad3ba4cd65e2eaaa3d64d40b.png" width=500/></center>

In the upcoming code, we shall rebuild our RAG applicaton with Tracing which will help us in evaluating across the different parameters that we discussed previously.

#### RUN THE FOLLOWING CODE & CONTINUE WITH THE NEXT SECTION

NOTE: THE FOLLOWING CODE BLOCK WAS COVERED IN THE PREVIOUS CLASSES. (same code used in the previous class)
"""

# # Installing required packages
# !pip install langchainhub
# !pip install langchain-openai
# !pip install langchain
# !pip install langchain-community
# !pip install faiss-cpu
# !pip install langchain_community
# !pip install openai
# !pip install -U langsmith

# Necessary Imports
import csv
import pandas as pd
import math
import numpy as np
import os
import getpass
from langchain_core.output_parsers import StrOutputParser
from google.colab import drive

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import create_openai_functions_agent

# Perform necessary imports for creating agents
from langchain import hub  # Used to pull predefined prompts from LangChain Hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain.memory import ChatMessageHistory  # Used to store chat history in memory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import OpenAI

# Setting up Open AI key
os.environ["OPENAI_API_KEY"] = getpass.getpass()

## Langsmith environment variables
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()

# Loading the data
drive.mount("/content/gdrive")
# df = pd.read_csv("/content/gdrive/MyDrive/datasets/sample_dataset.csv", index_col=0)
df = pd.read_csv(
    "prod_small.csv",
    index_col=0,
)  # sample_dataset.csv

# Preparing the Product Description
product_description = []
product_description_len = []
cnt = 0
for row in df.iterrows():
    cnt += 1
    if cnt == 100:
        break
    product = ""
    title = row[1]["TITLE"]
    if type(title) != float or not math.isnan(title):
        product += "Title\n" + title + "\n"
    description = row[1]["DESCRIPTION"]
    if type(description) != float or not math.isnan(description):
        product += "Description\n" + description + "\n"
    added_content = title or description
    if added_content:
        product = product.strip()
        product_description.append(product)
        product_description_len.append(len(product))


text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=250,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)
documents = text_splitter.create_documents(product_description)

# Create an embedding model using LangChain.
# See https://python.langchain.com/docs/integrations/text_embedding/ for a list of available embedding models on LangChain
embeddings = OpenAIEmbeddings()

# Create a vector store using the created chuns and the embeddings model
vector = FAISS.from_documents(documents, embeddings)

# Create ChatOpenAI object for acting as an LLM.
# See https://python.langchain.com/docs/integrations/chat/openai/
llm = ChatOpenAI(api_key=os.environ["OPENAI_API_KEY"], model="gpt-4o", temperature=0)

# Creating a retriever
# See https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/
retriever = vector.as_retriever()


# Importing the Client class from langsmith for managing interactions with the LangSmith platform
from langsmith import Client

# Importing the traceable decorator from langsmith to enable tracing of function calls
from langsmith import traceable

# Importing Annotated from typing_extensions to provide additional metadata to function parameters
from typing_extensions import Annotated

# Importing TypedDict from typing_extensions to define a structured dictionary with typed keys and values
from typing_extensions import TypedDict

## BUILD A RAG BOT AND TRACE USING LANGSMITH


# Add decorator so this function is traced in LangSmith
@traceable()
def rag_bot(question: str) -> dict:
    # langchain Retriever will be automatically traced
    docs = retriever.invoke(question)

    docs_string = "\n".join(doc.page_content for doc in docs)
    instructions = f"""You are a helpful assistant who is good at analyzing source information and answering questions.
    Use the following source documents retrieved from a knowledge base trained on amazon products to answer the user's questions{question}.

    If you don't know the answer, just say that you don't know.
     Documents:{docs_string}"""
    # langchain ChatModel will be automatically traced
    ai_msg = llm.invoke(
        [
            {"role": "system", "content": instructions},
            {"role": "user", "content": question},
        ],
    )

    return {"answer": ai_msg.content, "documents": docs}


"""This function implements a Retrieval-Augmented Generation (RAG) bot that retrieves relevant documents from a knowledge base and uses an AI language model to generate a response. It is designed to work with LangSmith for automatic tracing and monitoring.

Key Components:

1. LangSmith Tracing (`@traceable()`)
    - The function is decorated with `@traceable()`, which allows LangSmith to automatically trace and log the function's execution.
    - This helps in debugging, performance tracking, and understanding how the function behaves over time.
2. Document Retrieval (`retriever.invoke(question)`)
    - The function uses a retriever to fetch relevant documents from a knowledge base based on the user’s question.
    - The retriever is a part of LangChain, a framework designed for working with LLMs.
3. Formatting Retrieved Documents (`docs_string`)
    - The retrieved documents are combined into a single string to provide the AI model with contextual information.
    - This ensures the model has access to relevant information before generating an answer.
4. AI Model Instructions (`instructions`)
    - A system prompt is created to guide the AI model. It includes:
            - A description of the AI's role.
            - A directive to use the retrieved documents to answer the question.
            - A fallback instruction to indicate uncertainty if the answer is unknown.
5. LLM Invocation (`llm.invoke()`)
    - The AI model (likely from OpenAI, Anthropic, or another provider) is called using `llm.invoke()`.
    - The function sends two messages:
          - System message: Provides instructions on how the LLM should respond.
          - User message: Contains the actual question.
6. Returning the Response (`return {"answer": ..., "documents": ...}`)
    - The function returns a dictionary containing:
          - The LLM generated answer based on retrieved documents.
          - The retrieved documents for reference, which can be used for debugging or verification.

**How This Works in Practice?**

When a user asks a question, the function first retrieves relevant information.
The retrieved data is fed into the LLM model to ground its response in factual information rather than relying purely on generative capabilities.

The use of LangSmith tracing ensures visibility into how data flows through the function, making it easier to debug and optimize.
"""

res = rag_bot("what are some of the best shoes available?")

print("Answer\n", res['answer'])
print("Documents\n", res['documents'])

"""With the RAG application loaded, let's go ahead and build a RAG evaluation workflow.

The RAG evaluation workflow consists of three main steps:

- Creating a dataset with questions and their expected answers
- Running your RAG application on those questions
- Using evaluators to measure how well your application performed, looking at factors like:
   - Correctness
   - Relevance
   - Groundedness
   - Retrieval relevance

Let's go ahead and create:
- the eval set
- the evaluators for the above parameters
- run the application and calculate those metrics

Evaluation Set

We will start off with a very simple dataset containing 3 questions
"""

client = Client()

# Define the examples for the dataset
examples = [
    (
        "What are some of the best shoes in Amazon? ",
        "Some top shoes on Amazon include the adidas Men's Predator 18+ FG for firm ground soccer, the stylish PUMA Cali Sport Clean for women, and the elegant Kenneth Cole REACTION Crespo Loafer for men.",
    ),
    (
        "what are some of the backsplash wallpaper products available?",
        "Some backsplash wallpaper options include the Delavala Self-Adhesive Kitchen Backsplash Wallpaper for a stylish and easy-to-apply kitchen upgrade",
    ),
    (
        "What are some of the best kitchenware?",
        "Some of the best kitchenware options include the Prigo Enterprise Multipurpose Fridge Storage Containers for organized food storage and the HINS Elegant Looking Pot with Stand for a stylish and functional addition to your kitchen.",
    ),
]

# Create the dataset and examples in LangSmith
dataset_name = "amazon_products_dataset"
if not client.has_dataset(dataset_name=dataset_name):
    dataset = client.create_dataset(dataset_name=dataset_name)
    client.create_examples(
        inputs=[{"question": q} for q, _ in examples],
        outputs=[{"answer": a} for _, a in examples],
        dataset_id=dataset.id,
    )

"""### Code Explanation:
1. Initialize LangSmith Client:
     - Establish a connection with LangSmith using the Client().
     - This client manages dataset creation, storing examples, and tracking LLM performance.
2. Define Example Queries and Responses:
     - A list of question-answer pairs is created for testing the LLM model.
     - Each entry includes:
          - User's question (e.g., “What are some of the best shoes in Amazon?”).
          - LLM-generated response with product recommendations.
3. Check for Existing Dataset & create one if not available:
     - If the dataset exists, the script does not create a duplicate.
     - If the dataset does not exist, it is created under the specified name.
     - The dataset serves as a storage repository for structured query-response pairs.
4. Add Example Queries and Responses to the Dataset:
     - Each question is stored as an input (e.g., `{"question": "What are some of the best kitchenware?"}`).
     - Each corresponding answer is stored as an output (e.g., `{"answer": "Some of the best kitchenware options include..."}`).
     - These structured examples are uploaded to LangSmith, linked to the dataset.

#### Correctness Evaluator

Correctness: Response vs Reference Answer

- Goal: Measure how similar/correct is the RAG chain answer, relative to a ground-truth answer.
- Mode: Requires a ground truth (reference) answer supplied through a dataset.
- Evaluator: Use LLM-as-judge to assess answer correctness.
"""


# Grade output schema
class CorrectnessGrade(TypedDict):
    # Note that the order in the fields are defined is the order in which the model will generate them.
    # It is useful to put explanations before responses because it forces the model to think through
    # its final response before generating it:
    explanation: Annotated[str, ..., "Explain your reasoning for the score"]
    correct: Annotated[bool, ..., "True if the answer is correct, False otherwise."]


# Grade prompt
correctness_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER.

Here is the grade criteria to follow:
(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer.
(2) Ensure that the student answer does not contain any conflicting statements.
(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.

Correctness:
A correctness value of True means that the student's answer meets all of the criteria.
A correctness value of False means that the student's answer does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.

Avoid simply stating the correct answer at the outset.
"""

# Grader LLM
grader_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(
    CorrectnessGrade, method="json_schema", strict=True
)


def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    """An evaluator for RAG answer accuracy"""

    answers = f"""      QUESTION: {inputs['question']}
GROUND TRUTH ANSWER: {reference_outputs['answer']}
STUDENT ANSWER: {outputs['answer']}"""

    # Run evaluator
    grade = grader_llm.invoke(
        [
            {"role": "system", "content": correctness_instructions},
            {"role": "user", "content": answers},
        ]
    )
    return grade["correct"]


## Only for explaining the idea ... it is not used
from langchain_core.pydantic_v1 import BaseModel, Field


def test_correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    """An evaluator for RAG answer accuracy"""
    answers = f"""      QUESTION: {inputs['question']}
GROUND TRUTH ANSWER: {reference_outputs['answer']}
STUDENT ANSWER: {outputs['answer']}"""

    # Run evaluator
    raw_grader_llm = ChatOpenAI(model="gpt-4o", temperature=0)
    raw_grade = raw_grader_llm.invoke(
        [
            {"role": "system", "content": correctness_instructions},
            {"role": "user", "content": answers},
        ]
    )
    print("raw_grade\n", raw_grade.content)
    grade = grader_llm.invoke(
        [
            {"role": "system", "content": correctness_instructions},
            {"role": "user", "content": answers},
        ]
    )
    print(type(grade))
    print(grade)
    return grade["correct"]


test_input = {
    "question": "what are some of the backsplash wallpaper products available?"
}
test_outputs = {
    "answer": """
    Some available backsplash wallpaper products include:

1. Delavala Self Adhesive Kitchen Backsplash Wallpaper - Oil Proof Aluminum Foil Kitchen Sticker (Silver, 5 Meters).

If you need more specific products or options, I don't have that information.
"""
}

test_reference_outputs = {
    "answer": """
    Some backsplash wallpaper options include the Delavala Self-Adhesive Kitchen Backsplash Wallpaper for a stylish and easy-to-apply kitchen upgrade
    """
}
test_result = test_correctness(test_input, test_outputs, test_reference_outputs)

print(test_result)

"""#### What does the above code do?
- **Defines a grading schema (`CorrectnessGrade`)** using `TypedDict` to enforce structured output with two fields:
  - `explanation`: A string where the LLM explains its reasoning.
  - `correct`: A boolean indicating whether the student’s answer is factually correct.

- **Provides grading instructions (`correctness_instructions`)** for the LLM to follow, ensuring:
  - Answers are graded solely based on factual accuracy compared to the ground truth.
  - No conflicting statements are allowed in the student’s response.
  - Additional details beyond the ground truth are acceptable if factually accurate.
  - The model explains its reasoning step-by-step before determining correctness.

- **Initializes an LLM (`grader_llm`)** using GPT-4o with:
  - `temperature=0` for deterministic responses.
  - `.with_structured_output(CorrectnessGrade, method="json_schema", strict=True)` to enforce output consistency.

- **Defines the `correctness` function**, which:
  - Takes `inputs` (question), `outputs` (student's answer), and `reference_outputs` (ground truth).
  - Formats them into a structured prompt.
  - Invokes the LLM to evaluate correctness based on the grading criteria.
  - Extracts and returns the `correct` boolean value as the final result.

- **Purpose**: Automates grading for RAG-based answer evaluation, ensuring factual accuracy with structured, explainable feedback.

#### Relevance Evaluator

Relevance: Response vs Input

- Goal: Measure how well does the generated response address the initial user input.
- Mode: Does not require a reference answer, as it compares the response to the input question.
- Evaluator: Use LLM-as-judge to assess answer relevance, helpfulness, etc
"""


# Grade output schema
class RelevanceGrade(TypedDict):
    explanation: Annotated[str, ..., "Explain your reasoning for the score"]
    relevant: Annotated[
        bool, ..., "Provide the score on whether the answer addresses the question"
    ]


# Grade prompt
relevance_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION and a STUDENT ANSWER.

Here is the grade criteria to follow:
(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION
(2) Ensure the STUDENT ANSWER helps to answer the QUESTION

Relevance:
A relevance value of True means that the student's answer meets all of the criteria.
A relevance value of False means that the student's answer does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.

Avoid simply stating the correct answer at the outset."""

# Grader LLM
relevance_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(
    RelevanceGrade, method="json_schema", strict=True
)


# Evaluator
def relevance(inputs: dict, outputs: dict) -> bool:
    """A simple evaluator for RAG answer helpfulness."""

    llm_grader_input = f"""      QUESTION: {inputs['question']}
STUDENT ANSWER: {outputs['answer']}"""

    grade = relevance_llm.invoke(
        [
            {"role": "system", "content": relevance_instructions},
            {"role": "user", "content": llm_grader_input},
        ]
    )
    return grade["relevant"]


"""#### What does the above code do?

- **Defines a grading schema (`RelevanceGrade`)** using `TypedDict` with two fields:
  - `explanation`: A string where the LLM explains its reasoning.
  - `relevant`: A boolean indicating whether the student’s answer is relevant to the question.

- **Provides grading instructions (`relevance_instructions`)** for the LLM to follow, ensuring:
  - The student’s answer is concise and directly addresses the question.
  - The response contributes meaningfully to answering the question.
  - A detailed, step-by-step explanation of the grading decision is provided.
  - The model does not simply state the correct answer at the outset.

- **Initializes an LLM (`relevance_llm`)** using GPT-4o with:
  - `temperature=0` for deterministic, repeatable outputs.
  - `.with_structured_output(RelevanceGrade, method="json_schema", strict=True)` to enforce output consistency.

- **Defines the `relevance` function**, which:
  - Takes `inputs` (question) and `outputs` (student's answer).
  - Formats them into a structured prompt for evaluation.
  - Invokes the LLM to assess relevance based on the grading criteria.
  - Extracts and returns the `relevant` boolean value as the final result.

- **Purpose**: Automates grading for RAG-based answer evaluation, ensuring student responses are **concise, relevant, and properly justified**.

#### Groundedness evaluator

Groundedness: Response vs Retrieved Docs

- Goal: Measure to what extent does the generated response agree with the retrieved context.
- Mode: Does not require a reference answer, as it compares the response to the retrieved context.
- Evaluator: Use LLM-as-judge to assess faithfulness, hallucinations, etc.
"""


# Grade output schema
class GroundedGrade(TypedDict):
    explanation: Annotated[str, ..., "Explain your reasoning for the score"]
    grounded: Annotated[
        bool, ..., "Provide the score on if the answer hallucinates from the documents"
    ]


# Grade prompt
grounded_instructions = """You are a teacher grading a quiz.

You will be given FACTS and a STUDENT ANSWER.

Here is the grade criteria to follow:
(1) Ensure the STUDENT ANSWER is grounded in the FACTS.
(2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.

Grounded:
A grounded value of True means that the student's answer meets all of the criteria.
A grounded value of False means that the student's answer does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.

Avoid simply stating the correct answer at the outset."""

# Grader LLM
grounded_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(
    GroundedGrade, method="json_schema", strict=True
)


# Evaluator
def groundedness(inputs: dict, outputs: dict) -> bool:
    """A simple evaluator for RAG answer groundedness."""
    doc_string = "".join(doc.page_content for doc in outputs["documents"])
    answer = f"""      FACTS: {doc_string}
STUDENT ANSWER: {outputs['answer']}"""
    grade = grounded_llm.invoke(
        [
            {"role": "system", "content": grounded_instructions},
            {"role": "user", "content": answer},
        ]
    )
    return grade["grounded"]


"""#### What does the above code do?

- **Defines a grading schema (`GroundedGrade`)** using `TypedDict` with two fields:
  - `explanation`: A string where the LLM explains its reasoning.
  - `grounded`: A boolean indicating whether the student's answer is fully based on the provided facts (i.e., no hallucination).

- **Provides grading instructions (`grounded_instructions`)** for the LLM to follow, ensuring:
  - The student’s answer is strictly based on the given **FACTS**.
  - No information is introduced that is outside the provided **FACTS** (i.e., no hallucination).
  - The model explains its reasoning step-by-step before concluding.
  - The model avoids stating the correct answer upfront.

- **Initializes an LLM (`grounded_llm`)** using GPT-4o with:
  - `temperature=0` for deterministic outputs.
  - `.with_structured_output(GroundedGrade, method="json_schema", strict=True)` to enforce structured and reliable responses.

- **Defines the `groundedness` function**, which:
  - Extracts document content (`doc.page_content`) from `outputs["documents"]`.
  - Formats the `FACTS` (extracted text) and `STUDENT ANSWER` into a structured prompt.
  - Invokes the LLM to assess whether the student’s response is **fully supported** by the given facts.
  - Extracts and returns the `grounded` boolean value as the final result.

- **Purpose**: Automates grading for RAG-based answers, ensuring responses **stay factually grounded in provided documents** without introducing hallucinated information.

#### Retrieval relevance evaluator

Relevance: Retrieved Docs vs Input

- Goal: Measure how relevant are my retrieved results for this query.
- Mode: Does not require a reference answer, as it compares the query to the retrieved context.
- Evaluator: Use LLM-as-judge to assess relevance.
"""


# Grade output schema
class RetrievalRelevanceGrade(TypedDict):
    explanation: Annotated[str, ..., "Explain your reasoning for the score"]
    relevant: Annotated[
        bool,
        ...,
        "True if the retrieved documents are relevant to the question, False otherwise",
    ]


# Grade prompt
retrieval_relevance_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION and a set of FACTS provided by the student.

Here is the grade criteria to follow:
(1) You goal is to identify FACTS that are completely unrelated to the QUESTION
(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant
(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met

Relevance:
A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.
A relevance value of False means that the FACTS are completely unrelated to the QUESTION.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.

Avoid simply stating the correct answer at the outset."""

# Grader LLM
retrieval_relevance_llm = ChatOpenAI(
    model="gpt-4o", temperature=0
).with_structured_output(RetrievalRelevanceGrade, method="json_schema", strict=True)


def retrieval_relevance(inputs: dict, outputs: dict) -> bool:
    """An evaluator for document relevance"""
    doc_string = "".join(doc.page_content for doc in outputs["documents"])
    answer = f"""      FACTS: {doc_string}
QUESTION: {inputs['question']}"""

    # Run evaluator
    grade = retrieval_relevance_llm.invoke(
        [
            {"role": "system", "content": retrieval_relevance_instructions},
            {"role": "user", "content": answer},
        ]
    )
    return grade["relevant"]


"""#### What does the above code do?

- **Defines a grading schema (`RetrievalRelevanceGrade`)** using `TypedDict` with two fields:
  - `explanation`: A string where the LLM explains its reasoning.
  - `relevant`: A boolean indicating whether the retrieved documents are relevant to the given question.

- **Provides grading instructions (`retrieval_relevance_instructions`)** for the LLM to follow, ensuring:
  - The goal is to identify **completely unrelated** facts.
  - If the retrieved facts contain **any** keywords or semantic meaning related to the question, they are considered relevant.
  - Some unrelated information is acceptable as long as the facts still contain **some relevance** to the question.
  - The model explains its reasoning step-by-step before concluding.
  - The model avoids stating the correct answer upfront.

- **Initializes an LLM (`retrieval_relevance_llm`)** using GPT-4o with:
  - `temperature=0` for deterministic outputs.
  - `.with_structured_output(RetrievalRelevanceGrade, method="json_schema", strict=True)` to ensure structured and reliable responses.

- **Defines the `retrieval_relevance` function**, which:
  - Extracts document content (`doc.page_content`) from `outputs["documents"]`.
  - Formats the **FACTS** (retrieved documents) and **QUESTION** into a structured prompt.
  - Invokes the LLM to determine whether the retrieved documents are relevant to the question.
  - Extracts and returns the `relevant` boolean value as the final result.

- **Purpose**: Automates grading for **retrieved document relevance in RAG pipelines**, ensuring that retrieved facts **contain meaningful connections** to the original question.

Run the experiments
"""


def target(inputs: dict) -> dict:
    return rag_bot(inputs["question"])


experiment_results = client.evaluate(
    target,
    data=dataset_name,
    evaluators=[correctness, groundedness, relevance, retrieval_relevance],
    experiment_prefix="rag-doc-relevance-June25",
    metadata={"version": "eval gpt-4o-mini"},
)

experiment_results.to_pandas()

"""You can also monitor it using the LangSmith link that gets generated. The link would redirect to the evaluation results as shown below:

### Interpretation of RAG Evaluation Results:

1. **Correctness (`feedback.correctness`)**
   - The model's answers are factually incorrect compared to the reference answers.
   - In all cases, correctness is `False`, indicating a failure to generate accurate responses.

2. **Groundedness (`feedback.groundedness`)**
   - All responses are marked as `True`, meaning the generated answers are based on the provided documents and do not introduce hallucinations.
   - This suggests that the retrieval process is bringing in relevant information.

3. **Relevance (`feedback.relevance`)**
   - Some answers (like "I don't know.") are marked as `False`, indicating they do not sufficiently address the question.
   - This suggests that even when information is retrieved, the model is not structuring it well in the response.

4. **Retrieval Relevance (`feedback.retrieval_relevance`)**
   - Retrieval is mostly relevant (`True`), meaning the system fetches documents that contain semantically related content.
   - However, despite good retrieval, the final answers still fail correctness and relevance checks.

---

### Ways to Improve the RAG Application:

1. **Improve Answer Generation**
   - The model should be trained or prompted to synthesize a meaningful response from the retrieved documents instead of defaulting to "I don't know."
   - Fine-tuning or using a prompt engineering approach (e.g., "Answer based only on retrieved documents") could help.

2. **Enhance Retrieval Precision**
   - While retrieval is relevant, it may still contain noise or overly broad information. Consider:
     - Improving document ranking (re-ranker models like Cohere Rerank or BM25).
     - Using more structured retrieval (e.g., metadata filtering).

3. **Filter Irrelevant Responses**
   - Introduce a check to reject "I don't know" if documents contain useful information.
   - Use confidence scoring to determine whether to return an answer.

4. **Use a Multi-Step Answer Verification**
   - Implement a pipeline where the generated response is checked against correctness before final output.
   - Use LLM verification mechanisms to validate the extracted response.
"""
